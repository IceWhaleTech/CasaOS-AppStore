name: ollama
services:
  ollama:
    image: ollama/ollama:0.9.5
    container_name: ollama
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 2G
    restart: unless-stopped
    ports:
      - target: 11434
        published: "11434"
        protocol: tcp
    volumes:
      - type: bind
        source: /DATA/AppData/$AppID/
        target: /root/.ollama
    x-casaos:
      ports:
        - container: "11434"
          description:
            en_US: Ollama API Port
            en_GB: Ollama API Port
            it_IT: Porta API Ollama
            nb_NO: Ollama API Port
            zh_CN: Ollama API端口
            ja_JP: Ollama APIポート
            ko_KR: Ollama API 포트
            fr_FR: Port API Ollama
            de_DE: Ollama API Port
            sv_SE: Ollama API Port
            el_GR: Θύρα API Ollama
            hr_HR: Ollama API Port
            pt_PT: Porta API Ollama
            ru_RU: API порт Ollama
            tr_TR: Ollama API Portu
      volumes:
        - container: /root/.ollama
          description:
            en_US: Ollama Models and Data Directory
            en_GB: Ollama Models and Data Directory
            it_IT: Directory Modelli e Dati Ollama
            nb_NO: Ollama Modeller og Data-katalog
            zh_CN: Ollama模型和数据目录
            ja_JP: Ollamaモデルとデータディレクトリ
            ko_KR: Ollama 모델 및 데이터 디렉터리
            fr_FR: Répertoire des Modèles et Données Ollama
            de_DE: Ollama Modelle und Datenverzeichnis
            sv_SE: Ollama Modeller och Data-katalog
            el_GR: Κατάλογος Μοντέλων και Δεδομένων Ollama
            hr_HR: Ollama Modeli i Direktorij Podataka
            pt_PT: Diretório de Modelos e Dados Ollama
            ru_RU: Каталог моделей и данных Ollama
            tr_TR: Ollama Modeller ve Veri Dizini
x-casaos:
  architectures:
    - amd64
    - arm64
  main: ollama
  category: Chat
  developer: ollama
  author: ollama
  tagline:
    en_US: Get up and running with large language models locally
    en_GB: Get up and running with large language models locally
    it_IT: Avvia e usa grandi modelli linguistici localmente
    nb_NO: Kom i gang med store språkmodeller lokalt
    zh_CN: 在本地运行大型语言模型
    ja_JP: 大規模言語モデルをローカルで実行
    ko_KR: 로컬에서 대형 언어 모델 실행
    fr_FR: Démarrez avec de grands modèles de langage localement
    de_DE: Große Sprachmodelle lokal ausführen
    sv_SE: Kom igång med stora språkmodeller lokalt
    el_GR: Ξεκινήστε με μεγάλα γλωσσικά μοντέλα τοπικά
    hr_HR: Pokrenite velike jezične modele lokalno
    pt_PT: Execute grandes modelos de linguagem localmente
    ru_RU: Запуск больших языковых моделей локально
    tr_TR: Büyük dil modellerini yerel olarak çalıştırın
  description:
    en_US: |
      Ollama is a tool for running large language models locally, designed to help users quickly deploy and manage AI models via a simple command-line interface and server. Its intuitive Web interface and efficient design make it ideal for developers, researchers, and AI enthusiasts working on local hardware.

      The tool's core features include local model execution and multi-model support. It enables running models like Llama 3, Mistral, and Gemma, with simple commands for downloading and switching models. All data processing occurs locally, ensuring privacy. Low resource usage optimizes model loading, allowing smooth operation on limited hardware.

      It offers a RESTful API for application integration and supports tool calling (e.g., Llama 3.1) for complex tasks. Model management via Modelfile bundles weights and configurations for ease of use. The tool's efficiency and user control deliver a modern local AI solution.

      **Key Features:**
      - **Local Execution**: Run LLMs directly on your hardware without internet dependency
      - **Multiple Model Support**: Access to dozens of pre-trained models including Llama 3, Mistral, Gemma, Code Llama, and more
      - **Easy Model Management**: Simple commands to pull, run, and manage different models
      - **API Integration**: RESTful API for building applications and integrations
      - **Memory Efficient**: Optimized model loading and memory management
      - **Privacy-Focused**: All processing happens locally, ensuring data privacy

      **Supported Models:**
      - DeepSeek-R1 (1.5B, 7B, 8B, 14B, 32B, 70B, 671B parameters)
      - Gemma3n (2B, 4B parameters)
      - Gemma3 (1B, 4B, 12B, 27B parameters)
      - Qwen3 (0.6B, 1.7B, 4B, 8B, 14B, 30B, 32B, 235B parameters)
      - Qwen2.5vl (3B, 7B, 32B, 72B parameters)
      - Llama3.1 (8B, 70B, 405B parameters)
      - Llama3.2 (1B, 3B parameters)
      - Mistral (7B parameters)
      - And many more...

      **Use Cases:**
      - Local AI development and experimentation
      - Educational purposes and research
      - Building AI-powered applications
      - Code generation and assistance
      - Text generation and completion
      - Chatbots and conversational AI
      - Data analysis and insights

      **Learn More:**
      - [Ollama Official Website](https://ollama.com/)
      - [Ollama GitHub Repository](https://github.com/ollama/ollama)
      - [Model Library](https://ollama.com/library)
    en_GB: |
      Ollama is a tool for running large language models locally, designed to help users quickly deploy and manage AI models via a simple command-line interface and server. Its intuitive Web interface and efficient design make it ideal for developers, researchers, and AI enthusiasts working on local hardware.

      The tool's core features include local model execution and multi-model support. It enables running models like Llama 3, Mistral, and Gemma, with simple commands for downloading and switching models. All data processing occurs locally, ensuring privacy. Low resource usage optimises model loading, allowing smooth operation on limited hardware.

      It offers a RESTful API for application integration and supports tool calling (e.g., Llama 3.1) for complex tasks. Model management via Modelfile bundles weights and configurations for ease of use. The tool's efficiency and user control deliver a modern local AI solution.

      **Key Features:**
      - **Local Execution**: Run LLMs directly on your hardware without internet dependency
      - **Multiple Model Support**: Access to dozens of pre-trained models including Llama 3, Mistral, Gemma, Code Llama, and more
      - **Easy Model Management**: Simple commands to pull, run, and manage different models
      - **API Integration**: RESTful API for building applications and integrations
      - **Memory Efficient**: Optimised model loading and memory management
      - **Privacy-Focused**: All processing happens locally, ensuring data privacy

      **Supported Models:**
      - DeepSeek-R1 (1.5B, 7B, 8B, 14B, 32B, 70B, 671B parameters)
      - Gemma3n (2B, 4B parameters)
      - Gemma3 (1B, 4B, 12B, 27B parameters)
      - Qwen3 (0.6B, 1.7B, 4B, 8B, 14B, 30B, 32B, 235B parameters)
      - Qwen2.5vl (3B, 7B, 32B, 72B parameters)
      - Llama3.1 (8B, 70B, 405B parameters)
      - Llama3.2 (1B, 3B parameters)
      - Mistral (7B parameters)
      - And many more...

      **Use Cases:**
      - Local AI development and experimentation
      - Educational purposes and research
      - Building AI-powered applications
      - Code generation and assistance
      - Text generation and completion
      - Chatbots and conversational AI
      - Data analysis and insights

      **Learn More:**
      - [Ollama Official Website](https://ollama.com/)
      - [Ollama GitHub Repository](https://github.com/ollama/ollama)
      - [Model Library](https://ollama.com/library)
    it_IT: |
      Ollama è uno strumento per eseguire grandi modelli linguistici localmente, progettato per aiutare gli utenti a distribuire e gestire rapidamente modelli AI tramite una semplice interfaccia a riga di comando e server. La sua interfaccia Web intuitiva e il design efficiente lo rendono ideale per sviluppatori, ricercatori e appassionati di AI che lavorano su hardware locale.

      Le caratteristiche principali dello strumento includono l'esecuzione locale dei modelli e il supporto multi-modello. Consente di eseguire modelli come Llama 3, Mistral e Gemma, con comandi semplici per scaricare e cambiare modelli. Tutta l'elaborazione dei dati avviene localmente, garantendo la privacy. Il basso utilizzo delle risorse ottimizza il caricamento dei modelli, consentendo un funzionamento fluido su hardware limitato.

      Offre un'API RESTful per l'integrazione delle applicazioni e supporta le chiamate agli strumenti (ad esempio, Llama 3.1) per compiti complessi. La gestione dei modelli tramite Modelfile raggruppa pesi e configurazioni per facilità d'uso. L'efficienza dello strumento e il controllo dell'utente forniscono una soluzione AI locale moderna.

      **Caratteristiche Principali:**
      - **Esecuzione Locale**: Esegui LLM direttamente sul tuo hardware senza dipendenza da internet
      - **Supporto Multi-Modello**: Accesso a decine di modelli pre-addestrati inclusi Llama 3, Mistral, Gemma, Code Llama e altri
      - **Gestione Facile dei Modelli**: Comandi semplici per scaricare, eseguire e gestire diversi modelli
      - **Integrazione API**: API RESTful per costruire applicazioni e integrazioni
      - **Efficienza Memoria**: Caricamento ottimizzato dei modelli e gestione della memoria
      - **Focalizzato sulla Privacy**: Tutta l'elaborazione avviene localmente, garantendo la privacy dei dati

      **Modelli Supportati:**
      - DeepSeek-R1 (1.5B, 7B, 8B, 14B, 32B, 70B, 671B parametri)
      - Gemma3n (2B, 4B parametri)
      - Gemma3 (1B, 4B, 12B, 27B parametri)
      - Qwen3 (0.6B, 1.7B, 4B, 8B, 14B, 30B, 32B, 235B parametri)
      - Qwen2.5vl (3B, 7B, 32B, 72B parametri)
      - Llama3.1 (8B, 70B, 405B parametri)
      - Llama3.2 (1B, 3B parametri)
      - Mistral (7B parametri)
      - E molti altri...

      **Casi d'Uso:**
      - Sviluppo AI locale e sperimentazione
      - Scopi educativi e ricerca
      - Costruzione di applicazioni potenziate dall'AI
      - Generazione e assistenza codice
      - Generazione e completamento testo
      - Chatbot e AI conversazionale
      - Analisi dati e insights

      **Scopri di Più:**
      - [Sito Web Ufficiale Ollama](https://ollama.com/)
      - [Repository GitHub Ollama](https://github.com/ollama/ollama)
      - [Libreria Modelli](https://ollama.com/library)
    nb_NO: |
      Ollama er et verktøy for å kjøre store språkmodeller lokalt, designet for å hjelpe brukere med å raskt distribuere og administrere AI-modeller via et enkelt kommandolinjegrensesnitt og server. Det intuitive webgrensesnittet og effektive designet gjør det ideelt for utviklere, forskere og AI-entusiaster som jobber på lokal maskinvare.

      Verktøyets kjernefunksjoner inkluderer lokal modellkjøring og støtte for flere modeller. Det muliggjør kjøring av modeller som Llama 3, Mistral og Gemma, med enkle kommandoer for nedlasting og bytte av modeller. All databehandling skjer lokalt, noe som sikrer personvern. Lav ressursbruk optimaliserer modellastning, noe som tillater jevn drift på begrenset maskinvare.

      Det tilbyr et RESTful API for applikasjonsintegrasjon og støtter verktøykalling (f.eks. Llama 3.1) for komplekse oppgaver. Modellhåndtering via Modelfile samler vekter og konfigurasjoner for enkel bruk. Verktøyets effektivitet og brukerkontroll leverer en moderne lokal AI-løsning.

      **Hovedfunksjoner:**
      - **Lokal Kjøring**: Kjør LLMer direkte på maskinvaren din uten internettavhengighet
      - **Støtte for Flere Modeller**: Tilgang til dusinvis av forhåndstrente modeller inkludert Llama 3, Mistral, Gemma, Code Llama og mer
      - **Enkel Modellhåndtering**: Enkle kommandoer for å hente, kjøre og administrere forskjellige modeller
      - **API-integrasjon**: RESTful API for å bygge applikasjoner og integrasjoner
      - **Minneeffektiv**: Optimalisert modellastning og minnehåndtering
      - **Personvernfokusert**: All behandling skjer lokalt, noe som sikrer datapersonvern

      **Støttede Modeller:**
      - DeepSeek-R1 (1.5B, 7B, 8B, 14B, 32B, 70B, 671B parametere)
      - Gemma3n (2B, 4B parametere)
      - Gemma3 (1B, 4B, 12B, 27B parametere)
      - Qwen3 (0.6B, 1.7B, 4B, 8B, 14B, 30B, 32B, 235B parametere)
      - Qwen2.5vl (3B, 7B, 32B, 72B parametere)
      - Llama3.1 (8B, 70B, 405B parametere)
      - Llama3.2 (1B, 3B parametere)
      - Mistral (7B parametere)
      - Og mange flere...

      **Bruksområder:**
      - Lokal AI-utvikling og eksperimentering
      - Utdanningsformål og forskning
      - Bygging av AI-drevne applikasjoner
      - Kodegenerering og assistanse
      - Tekstgenerering og fullføring
      - Chatboter og samtale-AI
      - Dataanalyse og innsikt

      **Lær Mer:**
      - [Ollama Offisiell Nettside](https://ollama.com/)
      - [Ollama GitHub Repository](https://github.com/ollama/ollama)
      - [Modellbibliotek](https://ollama.com/library)
    zh_CN: |
      Ollama 是一款本地运行大型语言模型的工具，旨在通过简洁的命令行界面和服务器，帮助用户快速部署和管理 AI 模型。其直观的 Web 界面和高效设计，适合开发者、研究人员和 AI 爱好者在本地硬件上开展实验和应用开发。

      该工具的核心功能包括本地模型执行和多模型支持。它支持运行 Llama 3、Mistral、Gemma 等多种模型，用户可通过简单命令一键下载和切换模型。所有数据处理在本地完成，确保隐私保护。低资源占用优化了模型加载，使其在有限硬件上也能高效运行。

      它提供 RESTful API，方便与应用程序集成，支持工具调用（如 Llama 3.1），执行复杂任务。模型管理通过 Modelfile 整合权重和配置，操作简便。该工具以高效性和用户控制为核心，提供现代化的本地 AI 解决方案。

      **主要特性：**
      - **本地执行**：直接在您的硬件上运行LLMs，无需互联网依赖
      - **多模型支持**：访问数十个预训练模型，包括Llama 3、Mistral、Gemma、Code Llama等
      - **简易模型管理**：使用简单命令拉取、运行和管理不同模型
      - **API集成**：RESTful API用于构建应用程序和集成
      - **内存优化**：优化的模型加载和内存管理
      - **隐私保护**：所有处理都在本地进行，确保数据隐私

      **支持的模型：**
      - DeepSeek-R1 (1.5B, 7B, 8B, 14B, 32B, 70B, 671B参数)
      - Gemma3n (2B, 4B参数)
      - Gemma3 (1B, 4B, 12B, 27B参数)
      - Qwen3 (0.6B, 1.7B, 4B, 8B, 14B, 30B, 32B, 235B参数)
      - Qwen2.5vl (3B, 7B, 32B, 72B参数)
      - Llama3.1 (8B, 70B, 405B参数)
      - Llama3.2 (1B, 3B参数)
      - Mistral (7B参数)
      - 以及更多模型...

      **使用场景：**
      - 本地AI开发和实验
      - 教育目的和研究
      - 构建AI驱动的应用程序
      - 代码生成和辅助
      - 文本生成和补全
      - 聊天机器人和对话AI
      - 数据分析和洞察

      **了解更多：**
      - [Ollama 官方网站](https://ollama.com/)
      - [Ollama GitHub 仓库](https://github.com/ollama/ollama)
      - [模型库](https://ollama.com/library)
    ja_JP: |
      Ollamaは大規模言語モデルをローカルで実行するためのツールで、シンプルなコマンドラインインターフェースとサーバーを通じて、ユーザーが迅速にAIモデルをデプロイし管理できるよう設計されています。直感的なWebインターフェースと効率的な設計により、ローカルハードウェア上で作業する開発者、研究者、AIエンスージアストに最適です。

      このツールの核となる機能には、ローカルモデル実行とマルチモデルサポートが含まれます。Llama 3、Mistral、Gemmaなどのモデルを実行可能で、モデルのダウンロードと切り替えを簡単なコマンドで行えます。すべてのデータ処理がローカルで実行されるため、プライバシーが確保されます。低リソース使用量によりモデル読み込みが最適化され、限られたハードウェアでもスムーズな動作が可能です。

      アプリケーション統合用のRESTful APIを提供し、複雑なタスクのためのツール呼び出し（例：Llama 3.1）をサポートします。Modelfileを通じたモデル管理により、重みと設定が使いやすくバンドルされます。ツールの効率性とユーザー制御により、現代的なローカルAIソリューションを提供します。

      **主要機能：**
      - **ローカル実行**: インターネット依存なしでハードウェア上で直接LLMを実行
      - **マルチモデルサポート**: Llama 3、Mistral、Gemma、Code Llamaなど数十の事前学習済みモデルへのアクセス
      - **簡単なモデル管理**: 異なるモデルをプル、実行、管理するためのシンプルなコマンド
      - **API統合**: アプリケーションと統合を構築するためのRESTful API
      - **メモリ効率**: 最適化されたモデル読み込みとメモリ管理
      - **プライバシー重視**: すべての処理がローカルで実行され、データプライバシーを確保

      **サポートされているモデル：**
      - DeepSeek-R1 (1.5B, 7B, 8B, 14B, 32B, 70B, 671Bパラメータ)
      - Gemma3n (2B, 4Bパラメータ)
      - Gemma3 (1B, 4B, 12B, 27Bパラメータ)
      - Qwen3 (0.6B, 1.7B, 4B, 8B, 14B, 30B, 32B, 235Bパラメータ)
      - Qwen2.5vl (3B, 7B, 32B, 72Bパラメータ)
      - Llama3.1 (8B, 70B, 405Bパラメータ)
      - Llama3.2 (1B, 3Bパラメータ)
      - Mistral (7Bパラメータ)
      - その他多数...

      **使用例：**
      - ローカルAI開発と実験
      - 教育目的と研究
      - AI搭載アプリケーションの構築
      - コード生成と支援
      - テキスト生成と補完
      - チャットボットと対話AI
      - データ分析とインサイト

      **詳細情報：**
      - [Ollama公式ウェブサイト](https://ollama.com/)
      - [Ollama GitHubリポジトリ](https://github.com/ollama/ollama)
      - [モデルライブラリ](https://ollama.com/library)
    ko_KR: |
      Ollama는 간단한 명령줄 인터페이스와 서버를 통해 사용자가 AI 모델을 빠르게 배포하고 관리할 수 있도록 설계된 대형 언어 모델을 로컬에서 실행하는 도구입니다. 직관적인 웹 인터페이스와 효율적인 설계로 로컬 하드웨어에서 작업하는 개발자, 연구자, AI 애호가에게 이상적입니다.

      이 도구의 핵심 기능에는 로컬 모델 실행과 다중 모델 지원이 포함됩니다. Llama 3, Mistral, Gemma와 같은 모델을 실행할 수 있으며, 간단한 명령으로 모델을 다운로드하고 전환할 수 있습니다. 모든 데이터 처리가 로컬에서 발생하여 개인정보를 보장합니다. 낮은 리소스 사용량으로 모델 로딩을 최적화하여 제한된 하드웨어에서도 원활한 작동을 가능하게 합니다.

      애플리케이션 통합을 위한 RESTful API를 제공하고 복잡한 작업을 위한 도구 호출(예: Llama 3.1)을 지원합니다. Modelfile을 통한 모델 관리로 가중치와 구성을 쉽게 사용할 수 있도록 번들화합니다. 도구의 효율성과 사용자 제어는 현대적인 로컬 AI 솔루션을 제공합니다.

      **주요 기능:**
      - **로컬 실행**: 인터넷 의존성 없이 하드웨어에서 직접 LLM 실행
      - **다중 모델 지원**: Llama 3, Mistral, Gemma, Code Llama 등 수십 개의 사전 훈련된 모델에 대한 액세스
      - **쉬운 모델 관리**: 다양한 모델을 가져오고, 실행하고, 관리하는 간단한 명령
      - **API 통합**: 애플리케이션과 통합을 구축하기 위한 RESTful API
      - **메모리 효율성**: 최적화된 모델 로딩 및 메모리 관리
      - **개인정보 중심**: 모든 처리가 로컬에서 발생하여 데이터 개인정보 보장

      **지원되는 모델:**
      - DeepSeek-R1 (1.5B, 7B, 8B, 14B, 32B, 70B, 671B 매개변수)
      - Gemma3n (2B, 4B 매개변수)
      - Gemma3 (1B, 4B, 12B, 27B 매개변수)
      - Qwen3 (0.6B, 1.7B, 4B, 8B, 14B, 30B, 32B, 235B 매개변수)
      - Qwen2.5vl (3B, 7B, 32B, 72B 매개변수)
      - Llama3.1 (8B, 70B, 405B 매개변수)
      - Llama3.2 (1B, 3B 매개변수)
      - Mistral (7B 매개변수)
      - 그리고 더 많은 모델들...

      **사용 사례:**
      - 로컬 AI 개발 및 실험
      - 교육 목적 및 연구
      - AI 기반 애플리케이션 구축
      - 코드 생성 및 지원
      - 텍스트 생성 및 완성
      - 챗봇 및 대화형 AI
      - 데이터 분석 및 인사이트

      **더 알아보기:**
      - [Ollama 공식 웹사이트](https://ollama.com/)
      - [Ollama GitHub 저장소](https://github.com/ollama/ollama)
      - [모델 라이브러리](https://ollama.com/library)
    fr_FR: |
      Ollama est un outil pour exécuter des grands modèles de langage localement, conçu pour aider les utilisateurs à déployer et gérer rapidement des modèles IA via une interface en ligne de commande simple et un serveur. Son interface Web intuitive et sa conception efficace le rendent idéal pour les développeurs, chercheurs et passionnés d'IA travaillant sur du matériel local.

      Les fonctionnalités principales de l'outil incluent l'exécution de modèles locaux et le support multi-modèles. Il permet d'exécuter des modèles comme Llama 3, Mistral et Gemma, avec des commandes simples pour télécharger et changer de modèles. Tout le traitement des données se fait localement, assurant la confidentialité. Une faible utilisation des ressources optimise le chargement des modèles, permettant un fonctionnement fluide sur du matériel limité.

      Il offre une API RESTful pour l'intégration d'applications et prend en charge l'appel d'outils (par exemple, Llama 3.1) pour des tâches complexes. La gestion des modèles via Modelfile regroupe les poids et configurations pour faciliter l'utilisation. L'efficacité de l'outil et le contrôle utilisateur offrent une solution IA locale moderne.

      **Fonctionnalités Clés :**
      - **Exécution Locale** : Exécutez des LLM directement sur votre matériel sans dépendance internet
      - **Support Multi-Modèles** : Accès à des dizaines de modèles pré-entraînés incluant Llama 3, Mistral, Gemma, Code Llama et plus
      - **Gestion Facile des Modèles** : Commandes simples pour télécharger, exécuter et gérer différents modèles
      - **Intégration API** : API RESTful pour construire des applications et intégrations
      - **Efficacité Mémoire** : Chargement optimisé des modèles et gestion de la mémoire
      - **Axé sur la Confidentialité** : Tout le traitement se fait localement, assurant la confidentialité des données

      **Modèles Supportés :**
      - DeepSeek-R1 (1.5B, 7B, 8B, 14B, 32B, 70B, 671B paramètres)
      - Gemma3n (2B, 4B paramètres)
      - Gemma3 (1B, 4B, 12B, 27B paramètres)
      - Qwen3 (0.6B, 1.7B, 4B, 8B, 14B, 30B, 32B, 235B paramètres)
      - Qwen2.5vl (3B, 7B, 32B, 72B paramètres)
      - Llama3.1 (8B, 70B, 405B paramètres)
      - Llama3.2 (1B, 3B paramètres)
      - Mistral (7B paramètres)
      - Et bien plus...

      **Cas d'Usage :**
      - Développement IA local et expérimentation
      - Objectifs éducatifs et recherche
      - Construction d'applications alimentées par l'IA
      - Génération et assistance de code
      - Génération et complétion de texte
      - Chatbots et IA conversationnelle
      - Analyse de données et insights

      **En Savoir Plus :**
      - [Site Web Officiel Ollama](https://ollama.com/)
      - [Dépôt GitHub Ollama](https://github.com/ollama/ollama)
      - [Bibliothèque de Modèles](https://ollama.com/library)
    de_DE: |
      Ollama ist ein Tool zur lokalen Ausführung großer Sprachmodelle, das entwickelt wurde, um Benutzern zu helfen, AI-Modelle schnell über eine einfache Befehlszeilenschnittstelle und einen Server bereitzustellen und zu verwalten. Die intuitive Weboberfläche und das effiziente Design machen es ideal für Entwickler, Forscher und AI-Enthusiasten, die auf lokaler Hardware arbeiten.

      Die Kernfunktionen des Tools umfassen lokale Modellausführung und Multi-Modell-Unterstützung. Es ermöglicht die Ausführung von Modellen wie Llama 3, Mistral und Gemma mit einfachen Befehlen zum Herunterladen und Wechseln von Modellen. Alle Datenverarbeitung erfolgt lokal und gewährleistet Datenschutz. Geringe Ressourcennutzung optimiert das Laden von Modellen und ermöglicht reibungslosen Betrieb auf begrenzter Hardware.

      Es bietet eine RESTful API für Anwendungsintegration und unterstützt Tool-Aufrufe (z.B. Llama 3.1) für komplexe Aufgaben. Modellverwaltung über Modelfile bündelt Gewichtungen und Konfigurationen für einfache Nutzung. Die Effizienz des Tools und Benutzerkontrolle liefern eine moderne lokale AI-Lösung.

      **Hauptfunktionen:**
      - **Lokale Ausführung**: LLMs direkt auf Ihrer Hardware ohne Internetabhängigkeit ausführen
      - **Multi-Modell-Unterstützung**: Zugang zu Dutzenden von vortrainierten Modellen einschließlich Llama 3, Mistral, Gemma, Code Llama und mehr
      - **Einfache Modellverwaltung**: Einfache Befehle zum Ziehen, Ausführen und Verwalten verschiedener Modelle
      - **API-Integration**: RESTful API zum Erstellen von Anwendungen und Integrationen
      - **Speichereffizient**: Optimiertes Modelladen und Speicherverwaltung
      - **Datenschutzorientiert**: Alle Verarbeitung erfolgt lokal und gewährleistet Datenschutz

      **Unterstützte Modelle:**
      - DeepSeek-R1 (1.5B, 7B, 8B, 14B, 32B, 70B, 671B Parameter)
      - Gemma3n (2B, 4B Parameter)
      - Gemma3 (1B, 4B, 12B, 27B Parameter)
      - Qwen3 (0.6B, 1.7B, 4B, 8B, 14B, 30B, 32B, 235B Parameter)
      - Qwen2.5vl (3B, 7B, 32B, 72B Parameter)
      - Llama3.1 (8B, 70B, 405B Parameter)
      - Llama3.2 (1B, 3B Parameter)
      - Mistral (7B Parameter)
      - Und viele mehr...

      **Anwendungsfälle:**
      - Lokale AI-Entwicklung und Experimentierung
      - Bildungszwecke und Forschung
      - Aufbau AI-gestützter Anwendungen
      - Codegenerierung und Unterstützung
      - Textgenerierung und Vervollständigung
      - Chatbots und Konversations-AI
      - Datenanalyse und Erkenntnisse

      **Mehr Erfahren:**
      - [Ollama Offizielle Website](https://ollama.com/)
      - [Ollama GitHub Repository](https://github.com/ollama/ollama)
      - [Modellbibliothek](https://ollama.com/library)
    sv_SE: |
      Ollama är ett verktyg för att köra stora språkmodeller lokalt, designat för att hjälpa användare att snabbt distribuera och hantera AI-modeller via ett enkelt kommandoradsgränssnitt och server. Det intuitiva webbgränssnittet och effektiva designen gör det idealiskt för utvecklare, forskare och AI-entusiaster som arbetar på lokal hårdvara.

      Verktygets kärnfunktioner inkluderar lokal modellkörning och stöd för flera modeller. Det möjliggör körning av modeller som Llama 3, Mistral och Gemma, med enkla kommandon för nedladdning och växling av modeller. All databehandling sker lokalt, vilket säkerställer integritet. Låg resursanvändning optimerar modelladdning, vilket möjliggör smidig drift på begränsad hårdvara.

      Det erbjuder ett RESTful API för applikationsintegration och stöder verktygsanrop (t.ex. Llama 3.1) för komplexa uppgifter. Modellhantering via Modelfile bunttar vikter och konfigurationer för enkel användning. Verktygets effektivitet och användarkontroll levererar en modern lokal AI-lösning.

      **Huvudfunktioner:**
      - **Lokal Körning**: Kör LLMer direkt på din hårdvara utan internetberoende
      - **Stöd för Flera Modeller**: Åtkomst till dussintals förtränade modeller inklusive Llama 3, Mistral, Gemma, Code Llama och mer
      - **Enkel Modellhantering**: Enkla kommandon för att hämta, köra och hantera olika modeller
      - **API-integration**: RESTful API för att bygga applikationer och integrationer
      - **Minneseffektiv**: Optimerad modelladdning och minneshantering
      - **Integritetsfokuserad**: All bearbetning sker lokalt, vilket säkerställer dataintegritet

      **Stödda Modeller:**
      - DeepSeek-R1 (1.5B, 7B, 8B, 14B, 32B, 70B, 671B parametrar)
      - Gemma3n (2B, 4B parametrar)
      - Gemma3 (1B, 4B, 12B, 27B parametrar)
      - Qwen3 (0.6B, 1.7B, 4B, 8B, 14B, 30B, 32B, 235B parametrar)
      - Qwen2.5vl (3B, 7B, 32B, 72B parametrar)
      - Llama3.1 (8B, 70B, 405B parametrar)
      - Llama3.2 (1B, 3B parametrar)
      - Mistral (7B parametrar)
      - Och många fler...

      **Användningsfall:**
      - Lokal AI-utveckling och experimentering
      - Utbildningssyften och forskning
      - Byggande av AI-drivna applikationer
      - Kodgenerering och assistans
      - Textgenerering och komplettering
      - Chatbotar och konversations-AI
      - Dataanalys och insikter

      **Läs Mer:**
      - [Ollama Officiell Webbplats](https://ollama.com/)
      - [Ollama GitHub Repository](https://github.com/ollama/ollama)
      - [Modellbibliotek](https://ollama.com/library)
    el_GR: |
      Το Ollama είναι ένα εργαλείο για την τοπική εκτέλεση μεγάλων γλωσσικών μοντέλων, σχεδιασμένο για να βοηθήσει τους χρήστες να αναπτύξουν και να διαχειριστούν γρήγορα μοντέλα AI μέσω μιας απλής διεπαφής γραμμής εντολών και ενός διακομιστή. Η διαισθητική διεπαφή Ιστού και ο αποδοτικός σχεδιασμός του το καθιστούν ιδανικό για προγραμματιστές, ερευνητές και ενθουσιώδεις AI που εργάζονται σε τοπικό υλικό.

      Τα βασικά χαρακτηριστικά του εργαλείου περιλαμβάνουν τοπική εκτέλεση μοντέλων και υποστήριξη πολλαπλών μοντέλων. Επιτρέπει την εκτέλεση μοντέλων όπως τα Llama 3, Mistral και Gemma, με απλές εντολές για λήψη και εναλλαγή μοντέλων. Όλη η επεξεργασία δεδομένων γίνεται τοπικά, εξασφαλίζοντας απορρήτου. Η χαμηλή χρήση πόρων βελτιστοποιεί τη φόρτωση μοντέλων, επιτρέποντας ομαλή λειτουργία σε περιορισμένο υλικό.

      Προσφέρει ένα RESTful API για ενσωμάτωση εφαρμογών και υποστηρίζει κλήσεις εργαλείων (π.χ. Llama 3.1) για πολύπλοκες εργασίες. Η διαχείριση μοντέλων μέσω Modelfile συνδυάζει βάρη και διαμορφώσεις για ευκολία χρήσης. Η αποδοτικότητα του εργαλείου και ο έλεγχος χρήστη παραδίδουν μια σύγχρονη τοπική λύση AI.

      **Βασικά Χαρακτηριστικά:**
      - **Τοπική Εκτέλεση**: Εκτελέστε LLMs άμεσα στο υλικό σας χωρίς εξάρτηση από το διαδίκτυο
      - **Υποστήριξη Πολλαπλών Μοντέλων**: Πρόσβαση σε δεκάδες προεκπαιδευμένα μοντέλα συμπεριλαμβανομένων των Llama 3, Mistral, Gemma, Code Llama και περισσότερων
      - **Εύκολη Διαχείριση Μοντέλων**: Απλές εντολές για τη λήψη, εκτέλεση και διαχείριση διαφορετικών μοντέλων
      - **Ενσωμάτωση API**: RESTful API για τη δημιουργία εφαρμογών και ενσωματώσεων
      - **Αποδοτική Μνήμη**: Βελτιστοποιημένη φόρτωση μοντέλων και διαχείριση μνήμης
      - **Εστιασμένο στο Απόρρητο**: Όλη η επεξεργασία γίνεται τοπικά, εξασφαλίζοντας το απόρρητο δεδομένων

      **Υποστηριζόμενα Μοντέλα:**
      - DeepSeek-R1 (1.5B, 7B, 8B, 14B, 32B, 70B, 671B παράμετροι)
      - Gemma3n (2B, 4B παράμετροι)
      - Gemma3 (1B, 4B, 12B, 27B παράμετροι)
      - Qwen3 (0.6B, 1.7B, 4B, 8B, 14B, 30B, 32B, 235B παράμετροι)
      - Qwen2.5vl (3B, 7B, 32B, 72B παράμετροι)
      - Llama3.1 (8B, 70B, 405B παράμετροι)
      - Llama3.2 (1B, 3B παράμετροι)
      - Mistral (7B παράμετροι)
      - Και πολλά περισσότερα...

      **Περιπτώσεις Χρήσης:**
      - Τοπική ανάπτυξη AI και πειραματισμός
      - Εκπαιδευτικούς σκοπούς και έρευνα
      - Δημιουργία εφαρμογών με AI
      - Παραγωγή κώδικα και βοήθεια
      - Παραγωγή κειμένου και συμπλήρωση
      - Chatbots και συνομιλιακή AI
      - Ανάλυση δεδομένων και γνώσεις

      **Μάθετε Περισσότερα:**
      - [Επίσημη Ιστοσελίδα Ollama](https://ollama.com/)
      - [Αποθετήριο GitHub Ollama](https://github.com/ollama/ollama)
      - [Βιβλιοθήκη Μοντέλων](https://ollama.com/library)
    hr_HR: |
      Ollama je alat za pokretanje velikih jezičnih modela lokalno, dizajniran da pomogne korisnicima brzo implementirati i upravljati AI modelima putem jednostavnog sučelja naredbenog retka i servera. Njegovo intuitivno Web sučelje i efikasan dizajn čine ga idealnim za programere, istraživače i AI entuzijaste koji rade na lokalnom hardveru.

      Osnovne funkcije alata uključuju lokalno izvršavanje modela i podršku za više modela. Omogućuje pokretanje modela poput Llama 3, Mistral i Gemma, s jednostavnim naredbama za preuzimanje i prebacivanje modela. Sva obrada podataka događa se lokalno, osiguravajući privatnost. Niska upotreba resursa optimizira učitavanje modela, omogućujući glatko funkcioniranje na ograničenom hardveru.

      Nudi RESTful API za integraciju aplikacija i podržava pozivanje alata (npr. Llama 3.1) za složene zadatke. Upravljanje modelima putem Modelfile-a grupira težine i konfiguracije za lakše korištenje. Efikasnost alata i kontrola korisnika pružaju moderno lokalno AI rješenje.

      **Ključne Značajke:**
      - **Lokalno Izvršavanje**: Pokretajte LLM-ove direktno na svojem hardveru bez ovisnosti o internetu
      - **Podrška za Više Modela**: Pristup desetcima prethodno obučenih modela uključujući Llama 3, Mistral, Gemma, Code Llama i ostale
      - **Jednostavno Upravljanje Modelima**: Jednostavne naredbe za preuzimanje, pokretanje i upravljanje različitim modelima
      - **Integracija API-ja**: RESTful API za izgradnju aplikacija i integracija
      - **Efikasnost Memorije**: Optimizirano učitavanje modela i upravljanje memorijom
      - **Usmjereno na Privatnost**: Sva obrada se događa lokalno, osiguravajući privatnost podataka

      **Podržani Modeli:**
      - DeepSeek-R1 (1.5B, 7B, 8B, 14B, 32B, 70B, 671B parametara)
      - Gemma3n (2B, 4B parametara)
      - Gemma3 (1B, 4B, 12B, 27B parametara)
      - Qwen3 (0.6B, 1.7B, 4B, 8B, 14B, 30B, 32B, 235B parametara)
      - Qwen2.5vl (3B, 7B, 32B, 72B parametara)
      - Llama3.1 (8B, 70B, 405B parametara)
      - Llama3.2 (1B, 3B parametara)
      - Mistral (7B parametara)
      - I mnogi drugi...

      **Slučajevi Korištenja:**
      - Lokalni AI razvoj i eksperimentiranje
      - Obrazovne svrhe i istraživanje
      - Izgradnja AI-pokretanih aplikacija
      - Generiranje koda i pomoć
      - Generiranje teksta i dovršavanje
      - Chatbotovi i konverzacijska AI
      - Analiza podataka i uvidi

      **Saznajte Više:**
      - [Ollama Službena Web Stranica](https://ollama.com/)
      - [Ollama GitHub Repozitorij](https://github.com/ollama/ollama)
      - [Biblioteka Modela](https://ollama.com/library)
    pt_PT: |
      O Ollama é uma ferramenta para executar grandes modelos de linguagem localmente, concebida para ajudar os utilizadores a implementar e gerir rapidamente modelos de IA através de uma interface de linha de comandos simples e servidor. A sua interface Web intuitiva e design eficiente tornam-no ideal para programadores, investigadores e entusiastas de IA que trabalham em hardware local.

      As funcionalidades principais da ferramenta incluem execução local de modelos e suporte multi-modelo. Permite executar modelos como Llama 3, Mistral e Gemma, com comandos simples para descarregar e alternar modelos. Todo o processamento de dados ocorre localmente, garantindo privacidade. O baixo uso de recursos optimiza o carregamento de modelos, permitindo funcionamento suave em hardware limitado.

      Oferece uma API RESTful para integração de aplicações e suporta chamadas de ferramentas (ex.: Llama 3.1) para tarefas complexas. A gestão de modelos através do Modelfile agrupa pesos e configurações para facilidade de uso. A eficiência da ferramenta e controlo do utilizador fornecem uma solução de IA local moderna.

      **Funcionalidades Principais:**
      - **Execução Local**: Execute LLMs directamente no seu hardware sem dependência de internet
      - **Suporte Multi-Modelo**: Acesso a dezenas de modelos pré-treinados incluindo Llama 3, Mistral, Gemma, Code Llama e mais
      - **Gestão Fácil de Modelos**: Comandos simples para puxar, executar e gerir diferentes modelos
      - **Integração de API**: API RESTful para construir aplicações e integrações
      - **Eficiência de Memória**: Carregamento optimizado de modelos e gestão de memória
      - **Focado na Privacidade**: Todo o processamento acontece localmente, garantindo privacidade de dados

      **Modelos Suportados:**
      - DeepSeek-R1 (1.5B, 7B, 8B, 14B, 32B, 70B, 671B parâmetros)
      - Gemma3n (2B, 4B parâmetros)
      - Gemma3 (1B, 4B, 12B, 27B parâmetros)
      - Qwen3 (0.6B, 1.7B, 4B, 8B, 14B, 30B, 32B, 235B parâmetros)
      - Qwen2.5vl (3B, 7B, 32B, 72B parâmetros)
      - Llama3.1 (8B, 70B, 405B parâmetros)
      - Llama3.2 (1B, 3B parâmetros)
      - Mistral (7B parâmetros)
      - E muitos mais...

      **Casos de Uso:**
      - Desenvolvimento local de IA e experimentação
      - Propósitos educacionais e investigação
      - Construção de aplicações alimentadas por IA
      - Geração de código e assistência
      - Geração de texto e completação
      - Chatbots e IA conversacional
      - Análise de dados e insights

      **Saiba Mais:**
      - [Site Oficial do Ollama](https://ollama.com/)
      - [Repositório GitHub do Ollama](https://github.com/ollama/ollama)
      - [Biblioteca de Modelos](https://ollama.com/library)
    ru_RU: |
      Ollama — это инструмент для локального запуска больших языковых моделей, разработанный для быстрого развертывания и управления моделями ИИ через простой интерфейс командной строки и сервер. Интуитивный веб-интерфейс и эффективный дизайн делают его идеальным для разработчиков, исследователей и энтузиастов ИИ, работающих на локальном оборудовании.

      Основные функции инструмента включают локальное выполнение моделей и поддержку нескольких моделей. Он позволяет запускать модели такие как Llama 3, Mistral и Gemma с простыми командами для загрузки и переключения моделей. Вся обработка данных происходит локально, обеспечивая конфиденциальность. Низкое использование ресурсов оптимизирует загрузку моделей, позволяя плавную работу на ограниченном оборудовании.

      Предлагает RESTful API для интеграции приложений и поддерживает вызовы инструментов (например, Llama 3.1) для сложных задач. Управление моделями через Modelfile объединяет веса и конфигурации для удобства использования. Эффективность инструмента и пользовательский контроль обеспечивают современное локальное решение ИИ.

      **Ключевые Особенности:**
      - **Локальное Выполнение**: Запуск LLM непосредственно на вашем оборудовании без зависимости от интернета
      - **Поддержка Нескольких Моделей**: Доступ к десяткам предварительно обученных моделей включая Llama 3, Mistral, Gemma, Code Llama и другие
      - **Простое Управление Моделями**: Простые команды для загрузки, запуска и управления различными моделями
      - **Интеграция API**: RESTful API для создания приложений и интеграций
      - **Эффективность Памяти**: Оптимизированная загрузка моделей и управление памятью
      - **Ориентированность на Конфиденциальность**: Вся обработка происходит локально, обеспечивая конфиденциальность данных

      **Поддерживаемые Модели:**
      - DeepSeek-R1 (1.5B, 7B, 8B, 14B, 32B, 70B, 671B параметров)
      - Gemma3n (2B, 4B параметров)
      - Gemma3 (1B, 4B, 12B, 27B параметров)
      - Qwen3 (0.6B, 1.7B, 4B, 8B, 14B, 30B, 32B, 235B параметров)
      - Qwen2.5vl (3B, 7B, 32B, 72B параметров)
      - Llama3.1 (8B, 70B, 405B параметров)
      - Llama3.2 (1B, 3B параметров)
      - Mistral (7B параметров)
      - И многие другие...

      **Случаи Использования:**
      - Локальная разработка ИИ и экспериментирование
      - Образовательные цели и исследования
      - Создание приложений на основе ИИ
      - Генерация кода и помощь
      - Генерация текста и завершение
      - Чат-боты и разговорный ИИ
      - Анализ данных и инсайты

      **Узнать Больше:**
      - [Официальный Сайт Ollama](https://ollama.com/)
      - [GitHub Репозиторий Ollama](https://github.com/ollama/ollama)
      - [Библиотека Моделей](https://ollama.com/library)
    tr_TR: |
      Ollama, kullanıcıların basit bir komut satırı arayüzü ve sunucu aracılığıyla AI modellerini hızla dağıtmasına ve yönetmesine yardımcı olmak için tasarlanmış, büyük dil modellerini yerel olarak çalıştırmaya yönelik bir araçtır. Sezgisel Web arayüzü ve verimli tasarımı, yerel donanımda çalışan geliştiriciler, araştırmacılar ve AI meraklıları için ideal kılar.

      Aracın temel özellikleri yerel model yürütme ve çoklu model desteğini içerir. Llama 3, Mistral ve Gemma gibi modelleri çalıştırmayı mümkün kılar, modelleri indirmek ve değiştirmek için basit komutlarla. Tüm veri işleme yerel olarak gerçekleşir, gizliliği sağlar. Düşük kaynak kullanımı model yüklemeyi optimize eder, sınırlı donanımda sorunsuz çalışmaya olanak tanır.

      Uygulama entegrasyonu için RESTful API sunar ve karmaşık görevler için araç çağrılarını (örn. Llama 3.1) destekler. Modelfile aracılığıyla model yönetimi, kullanım kolaylığı için ağırlıkları ve yapılandırmaları paketler. Aracın verimliliği ve kullanıcı kontrolü modern yerel AI çözümü sunar.

      **Ana Özellikler:**
      - **Yerel Yürütme**: İnternet bağımlılığı olmadan donanımınızda doğrudan LLM'leri çalıştırın
      - **Çoklu Model Desteği**: Llama 3, Mistral, Gemma, Code Llama ve daha fazlasını içeren onlarca önceden eğitilmiş modele erişim
      - **Kolay Model Yönetimi**: Farklı modelleri çekmek, çalıştırmak ve yönetmek için basit komutlar
      - **API Entegrasyonu**: Uygulamalar ve entegrasyonlar oluşturmak için RESTful API
      - **Bellek Verimli**: Optimize edilmiş model yükleme ve bellek yönetimi
      - **Gizlilik Odaklı**: Tüm işleme yerel olarak gerçekleşir, veri gizliliğini sağlar

      **Desteklenen Modeller:**
      - DeepSeek-R1 (1.5B, 7B, 8B, 14B, 32B, 70B, 671B parametre)
      - Gemma3n (2B, 4B parametre)
      - Gemma3 (1B, 4B, 12B, 27B parametre)
      - Qwen3 (0.6B, 1.7B, 4B, 8B, 14B, 30B, 32B, 235B parametre)
      - Qwen2.5vl (3B, 7B, 32B, 72B parametre)
      - Llama3.1 (8B, 70B, 405B parametre)
      - Llama3.2 (1B, 3B parametre)
      - Mistral (7B parametre)
      - Ve daha fazlası...

      **Kullanım Durumları:**
      - Yerel AI geliştirme ve deneyim
      - Eğitim amaçları ve araştırma
      - AI destekli uygulamalar oluşturma
      - Kod üretimi ve yardım
      - Metin üretimi ve tamamlama
      - Chatbotar ve konuşma AI'ı
      - Veri analizi ve içgörüler

      **Daha Fazla Bilgi:**
      - [Ollama Resmi Web Sitesi](https://ollama.com/)
      - [Ollama GitHub Deposu](https://github.com/ollama/ollama)
      - [Model Kütüphanesi](https://ollama.com/library)
  icon: https://cdn.jsdelivr.net/gh/IceWhaleTech/CasaOS-AppStore@main/Apps/Ollama/icon.png
  screenshot_link:
    - https://cdn.jsdelivr.net/gh/IceWhaleTech/CasaOS-AppStore@main/Apps/Ollama/screenshot-1.png
    - https://cdn.jsdelivr.net/gh/IceWhaleTech/CasaOS-AppStore@main/Apps/Ollama/screenshot-2.png
    - https://cdn.jsdelivr.net/gh/IceWhaleTech/CasaOS-AppStore@main/Apps/Ollama/screenshot-3.png
  thumbnail: ""
  scheme: http
  port_map: "11434"
  index: /
  title:
    en_US: Ollama